{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 계단 함수의 그래프\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid, relu, softmax\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13, 16, 19])"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬 연산\n",
    "A = np.array([1,2])\n",
    "print(A.shape)\n",
    "B = np.array([[3,4,5], [5,6,7]])\n",
    "print(B.shape)\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)  # Flatten\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE, cross_entropy_error\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# numpy로 원-핫 인코딩\n",
    "num_classes  = 10\n",
    "t_train = np.eye(num_classes)[t_train]\n",
    "t_test = np.eye(num_classes)[t_test]\n",
    "\n",
    "print(t_train.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치화\n",
    "np.random.seed(42)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치용 CSE 구현\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "    \n",
    "    t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    print(y[np.arange(batch_size), t].shape)\n",
    "    print(y[np.arange(batch_size), t])\n",
    "    return -np.sum(t*np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[0.7 0.3 0.7]\n",
      "Cross Entropy Error: 0.3567\n"
     ]
    }
   ],
   "source": [
    "# 예측 확률 분포 (softmax 출력)\n",
    "y = np.array([[0.1, 0.7, 0.2],  # 첫 번째 샘플\n",
    "              [0.3, 0.4, 0.3],  # 두 번째 샘플\n",
    "              [0.2, 0.1, 0.7]]) # 세 번째 샘플\n",
    "\n",
    "# 실제 레이블 (원-핫 인코딩)\n",
    "t = np.array([[0, 1, 0],  # 첫 번째 샘플의 실제 클래스: 1\n",
    "              [1, 0, 0],  # 두 번째 샘플의 실제 클래스: 0\n",
    "              [0, 0, 1]]) # 세 번째 샘플의 실제 클래스: 2\n",
    "\n",
    "# 교차 엔트로피 손실 계산\n",
    "loss = cross_entropy_error(y, t)\n",
    "print(f\"Cross Entropy Error: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 미분\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "# 편미분\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp = x[idx]\n",
    "        x[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient_2d(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return numerical_gradient_1d(f, x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx, x in enumerate(x):\n",
    "            grad[idx] = numerical_gradient_1d(f, x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법법\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient_2d(f, x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수에서의 기울기기\n",
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr= 0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415 -0.1382643   0.64768854]\n",
      " [ 1.52302986 -0.23415337 -0.23413696]]\n"
     ]
    }
   ],
   "source": [
    "# 신경망에서의 기울기\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "    \n",
    "    t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.51645238 -0.27028128  0.20130356]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.8])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.353937033131167"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0.0, 0.0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83568861  0.1399832  -0.97567181]\n",
      " [ 1.11425148  0.18664427 -1.30089575]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient_2d(f, net.W)\n",
    "print(dW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "    \n",
    "    # t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = sigmoid(a2)\n",
    "        y = softmax(z2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accurracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        # t = np.argmax(t, axis=1)\n",
    "        print(y.shape, t.shape)\n",
    "        accuracy = np.sum((y == t)) / y.size\n",
    "        # accuracy = np.sum((y == t)) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient_2d(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient_2d(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient_2d(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient_2d(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "# net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "# x = np.random.rand(100, 784)\n",
    "# t = np.random.rand(100, 10)\n",
    "# y = net.predict(x)\n",
    "\n",
    "# print(net.accurracy(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = net.numerical_gradient(x, t)\n",
    "# grads['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# train_loss_list = []\n",
    "\n",
    "# iters_num = 10000\n",
    "# train_size = x_train.shape[0]\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# network = TwoLayerNet(input_size= 784, hidden_size= 50, output_size= 10)\n",
    "\n",
    "# for i in range(iters_num):\n",
    "\n",
    "#     batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     x_batch = x_train[batch_mask]\n",
    "#     t_batch = t_train[batch_mask]\n",
    "\n",
    "#     grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "#     for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "#         network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "#     loss = network.loss(x_batch, t_batch)\n",
    "#     train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# train_loss_list = []\n",
    "# train_acc_list = []\n",
    "# test_acc_list = []\n",
    "\n",
    "# iters_num = 10000\n",
    "# train_size = x_train.shape[0]\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# network = TwoLayerNet(input_size= 784, hidden_size= 50, output_size= 10)\n",
    "\n",
    "# iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# for i in range(iters_num):\n",
    "\n",
    "#     batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     x_batch = x_train[batch_mask]\n",
    "#     t_batch = t_train[batch_mask]\n",
    "\n",
    "#     grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "#     for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "#         network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "#     loss = network.loss(x_batch, t_batch)\n",
    "#     train_loss_list.append(loss)\n",
    "\n",
    "#     if i % iter_per_epoch == 0:\n",
    "#         train_acc = network.accurracy(x_train, t_train)\n",
    "#         test_acc = network.accurracy(x_test, t_test)\n",
    "#         train_acc_list.append(train_acc)\n",
    "#         test_acc_list.append(test_acc)\n",
    "#         print(\"train_acc test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. 오차역전파법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200.0\n"
     ]
    }
   ],
   "source": [
    "apple = 100.0\n",
    "apple_num = 2.0\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x+y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        dy = dout\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1+np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout , self.W.T)\n",
    "        # print(dx.shape, dout.shape, self.W.shape)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        # print(self.dW)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    x = x - np.max(x) \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx * dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 클래스\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, t.size)\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp = x[idx]\n",
    "        x[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient_2d(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return numerical_gradient_1d(f, x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx, row in enumerate(x):\n",
    "            grad[idx] = numerical_gradient_1d(f, row)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastlayers = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayers.forward(y, t)\n",
    "    \n",
    "    def accurracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        # print(y.shape, t.shape)\n",
    "        # accuracy = np.sum((y == t)) / y.size\n",
    "        accuracy = np.sum((y == t)) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient_2d(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient_2d(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient_2d(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient_2d(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.lastlayers.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (3, 28, 28)\n",
      "W1: 5.019833973364654e-10\n",
      "b1: 2.7025218238361375e-09\n",
      "W2: 5.7881141598110936e-09\n",
      "b2: 1.3945938001636016e-07\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# 원핫 벡터로 변환\n",
    "num_classes = 10\n",
    "t_train = np.eye(num_classes)[t_train]\n",
    "t_test = np.eye(num_classes)[t_test]\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size= 50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "print(x_train.shape, x_batch.shape)\n",
    "\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "# print(grad_backprop)\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "# print(grad_numerical)\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \": \" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc test acc | 0.10226666666666667, 0.1009\n",
      "train_acc test acc | 0.9036166666666666, 0.9055\n",
      "train_acc test acc | 0.9244666666666667, 0.9254\n",
      "train_acc test acc | 0.9370666666666667, 0.9368\n",
      "train_acc test acc | 0.9437666666666666, 0.9424\n",
      "train_acc test acc | 0.95245, 0.951\n",
      "train_acc test acc | 0.9548833333333333, 0.9529\n",
      "train_acc test acc | 0.9593833333333334, 0.957\n",
      "train_acc test acc | 0.96375, 0.9596\n",
      "train_acc test acc | 0.96675, 0.9601\n",
      "train_acc test acc | 0.96825, 0.9631\n",
      "train_acc test acc | 0.9701, 0.9655\n",
      "train_acc test acc | 0.9714166666666667, 0.9647\n",
      "train_acc test acc | 0.9726833333333333, 0.9663\n",
      "train_acc test acc | 0.9753166666666667, 0.9672\n",
      "train_acc test acc | 0.9756333333333334, 0.968\n",
      "train_acc test acc | 0.9778333333333333, 0.9699\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# 원핫 벡터로 변환\n",
    "num_classes = 10\n",
    "t_train = np.eye(num_classes)[t_train]\n",
    "t_test = np.eye(num_classes)[t_test]\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size= 50, output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accurracy(x_train, t_train)\n",
    "        test_acc = network.accurracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01) :\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]\n",
    "\n",
    "class Nesterov:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            prev_v = self.v[key]\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += -self.momentum * prev_v + (1 + self.momentum) * self.v[key]\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc test acc | 0.16891666666666666, 0.1666\n",
      "train_acc test acc | 0.9198666666666667, 0.921\n",
      "train_acc test acc | 0.9369333333333333, 0.9367\n",
      "train_acc test acc | 0.9475666666666667, 0.9435\n",
      "train_acc test acc | 0.95665, 0.9543\n",
      "train_acc test acc | 0.9616, 0.9587\n",
      "train_acc test acc | 0.9671333333333333, 0.9631\n",
      "train_acc test acc | 0.9701333333333333, 0.9647\n",
      "train_acc test acc | 0.9739, 0.9674\n",
      "train_acc test acc | 0.97625, 0.9687\n",
      "train_acc test acc | 0.9786333333333334, 0.9709\n",
      "train_acc test acc | 0.9802666666666666, 0.9708\n",
      "train_acc test acc | 0.9806666666666667, 0.9704\n",
      "train_acc test acc | 0.9810833333333333, 0.9696\n",
      "train_acc test acc | 0.98515, 0.9722\n",
      "train_acc test acc | 0.98505, 0.9711\n",
      "train_acc test acc | 0.987, 0.9728\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# 원핫 벡터로 변환\n",
    "num_classes = 10\n",
    "t_train = np.eye(num_classes)[t_train]\n",
    "t_test = np.eye(num_classes)[t_test]\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size= 50, output_size=10)\n",
    "# optimizer = SGD()\n",
    "# optimizer = Momentum()\n",
    "# optimizer = Nesterov()\n",
    "optimizer = Adam()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    #     network.params[key] -= learning_rate * grad[key]\n",
    "    optimizer.update(network.params, grads = grad)\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accurracy(x_train, t_train)\n",
    "        test_acc = network.accurracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 학습 관련 기술들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5  # 은닉층이 5개\n",
    "activations = {}  # 이곳에 활성화 결과를 저장\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "    # w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "    # 활성화 함수도 바꿔가며 실험해보자！\n",
    "    # z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "plt.figure(figsize=(12, 3))\n",
    "# 히스토그램 그리기\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0:\n",
    "        plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def smooth_curve(x):\n",
    "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
    "    \n",
    "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"데이터셋을 뒤섞는다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 훈련 데이터\n",
    "    t : 정답 레이블\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation, :] if x.ndim == 2 else x[permutation, :, :, :]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
